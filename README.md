# Multilingual RAG (Retrieval-Augmented Generation)

A Python-based system for multilingual document retrieval and question answering, supporting both Bangla and English queries.

---

## Setup Guide

1. **Clone the repository:**
   ```sh
   git clone https://github.com/ekramasif/multilingualRAG.git
   cd multilingualRAG
   ```

2. **Create and activate a virtual environment:**
    ```sh
    python -m venv venv

        # For Windows (CMD/PowerShell)
    .\venv\Scripts\activate

        # For Git Bash on Windows
    source venv/Scripts/activate

        # For macOS/Linux
    source venv/bin/activate
    ```

3. **Install dependencies:**
    ```sh
    pip install -r requirements.txt
    ```

4. **Add your documents:**
   Place your PDF or text files in the `data/` directory.

5. **Run the Application:**

    ```sh
    uvicorn main:app --reload
    ```

## Used Tools, Libraries, and Packages

### API Framework
- **fastapi**: A modern, high-performance web framework for building APIs.
- **uvicorn**: An ASGI server used to run the FastAPI application.

### AI & Machine Learning (LangChain Ecosystem)
- **langchain**: The core framework for orchestrating the RAG pipeline.
- **langchain-google-genai**: Provides integrations for Google's Gemini models.
- **langchain-community**: Contains community-contributed components like vector stores and retrievers.
- **google-generativeai**: The underlying Python SDK for the Gemini API.

### Vector Embeddings & Storage
- **sentence-transformers**: A library for generating state-of-the-art sentence and text embeddings.
- **faiss-cpu**: Facebook AI Similarity Search, a highly efficient library for vector similarity search and clustering.

### Data Handling & Environment
- **pymupdf**: An advanced library (fitz) for PDF text and table extraction.
- **python-dotenv**: For managing environment variables from a .env file.


## Sample Queries and Outputs

Below are examples of how to interact with the API and the expected responses.

### Sample 1 (Bengali)
- **Query:** কাকে অনুপমের ভাগ্য দেবতা বলে উল্লেখ করা হয়েছে?  
  *(Who is mentioned as the god of Anupam's destiny?)*
- **Expected Output:** মামা *(Uncle)*

### Sample 2 (Bengali)
- **Query:** বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল?  
  *(What was Kalyani's real age at the time of the wedding?)*
- **Expected Output:** পনেরো বছর *(Fifteen years)*

### Sample 3 (English)
- **Query:** Who is called a handsome man in Anupam's language?
- **Expected Output:** শম্ভুনাথ *(Shambhunath)*

---

## API Documentation

The API documentation is automatically generated by FastAPI and can be accessed at:  
[http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

- **Endpoint:** `POST /ask`
- **Description:** Accepts a question and returns a precise, context-based answer.

**Request Body:**
```json
{
  "question": "your question here"
}
```

**Success Response (200 OK):**
```json
{
  "answer": "The model's generated answer.",
  "source_page": 15
}
```

**Error Responses:**
- `503 Service Unavailable`: The RAG chain failed to initialize during server startup.
- `500 Internal Server Error`: An unexpected error occurred while processing the request.

---

## Evaluation Matrix (Conceptual Framework)

While not fully implemented in the code, the following metrics are crucial for a production-level evaluation of the RAG system:

| Metric              | Question it Answers                                   | How to Measure                                                                                  |
|---------------------|------------------------------------------------------|-------------------------------------------------------------------------------------------------|
| **Groundedness**    | Is the answer directly supported by the retrieved context? | Use a separate LLM call to "fact-check" the answer against the retrieved chunks.                |
| **Relevance**       | Did the system retrieve the most relevant chunks?     | Requires a labeled dataset; measure cosine similarity between query and retrieved/ideal chunks.  |
| **Answer Correctness** | Is the final answer correct?                       | Compare the model's answer against a "golden dataset" of correct answers.                       |

---

## Technical Questions & Answers

**Q1: What method or library did you use to extract the text, and why? Did you face any formatting challenges with the PDF content?**  
- **Method/Library:** PyMuPDF (fitz)  
- **Why:** PyMuPDF is more powerful than alternatives like pypdf, allowing parsing of PDF structure, including coordinates of text blocks and tables.  
- **Formatting Challenges:** Yes, due to visual layouts (columns, tables). The `process_pdf_smartly` function was designed to:
  - Identify all tables on a page.
  - Extract table data and convert to Markdown.
  - Extract regular text blocks, avoiding overlap with tables.

---

**Q2: What chunking strategy did you choose? Why do you think it works well for semantic retrieval?**  
- **Strategy:** RecursiveCharacterTextSplitter from LangChain  
- **Why:** It splits text hierarchically (paragraphs → sentences → words), preserving semantic integrity and local context for better retrieval.

---

**Q3: What embedding model did you use? Why did you choose it? How does it capture the meaning of the text?**  
- **Model:** `sentence-transformers/paraphrase-multilingual-mpnet-base-v2`  
- **Why:** Supports 50+ languages, including English and Bengali, mapping them into a shared vector space for effective cross-lingual retrieval.  
- **How:** Converts text to high-dimensional vectors; similar meanings yield close vectors (cosine similarity), capturing context, syntax, and relationships.

---

**Q4: How are you comparing the query with your stored chunks? Why did you choose this similarity method and storage setup?**  
- **Comparison and Storage:** FAISS (Facebook AI Similarity Search) as the vector store.  
- **Why:** FAISS enables fast, scalable similarity search using cosine similarity, which is standard for semantic embeddings.

---

**Q5: How do you ensure that the question and the document chunks are compared meaningfully? What would happen if the query is vague or missing context?**  
- **Meaningful Comparison:** Both queries and chunks use the same embedding model, ensuring a shared semantic space.  
- **Handling Vague Queries:** The MultiQueryRetriever uses an LLM (Gemini) to generate more specific, re-phrased questions, broadening the search and improving context.

---

**Q6: Do the results seem relevant? If not, what might improve them?**  
- **Relevance:** Results are highly relevant due to the synergy of PyMuPDF extraction, semantic chunking, multilingual embeddings, and multi-query retrieval.  
- **Potential Improvements:**
  - Cross-Encoder Reranking: Use a more powerful model to re-rank top retrieved documents.
  - Embedding Model Fine-Tuning: Fine-tune on domain-specific data (e.g., Bengali literature).
  - Advanced Chunking: Use LLMs for agentic chunking (splitting by complete ideas).
  - Graph-Based Retrieval: Build a knowledge graph for complex documents with many interconnected entities.
